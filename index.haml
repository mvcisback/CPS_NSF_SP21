!!! 5

%html
  %head
    %title NSF CPS PI Meeting

    %meta(charset="utf-8")/
    %meta(name="description" content="BDD / BAIR Poster Spring 2021.")/

    %link(rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css")
    %link(rel="stylesheet" href="poster.css" type="text/css")/

    %script(defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js")
    %script(defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);")


%body
  %header
    %img#seal(src="imgs/ucbseal_139_540.png")/

    %div#title
      %h1 Specifications from demonstrations; A Maximum Entropy Approach
      %div
        %ol
          %li <strong>Marcell Vazquez-Chanlatte</strong>
          %li Sanjit A. Seshia

    %div#nsf-code
      %p 
        NSF Grant </br> #1545126

    %div#qr-codes
      %figure
        %img(src="imgs/cav.svg")/
        %figcaption CAV 20'.

  %main
    %section#movitation
      %h1 Motivating Example

      %h2 What was the agent trying to do?
      %figure
        %figcaption
          Consider an agent acting in the following stochastic grid
          world.

        %img(src="imgs/enter_lava.svg")

        %figcaption
          <strong>Q: </strong>Did the agent intend to touch the <b>red</b> tile?

    %section#mdps
      %h1 Technical Construction

      %p 
        <strong>Key Observation:</strong> Can think of soft constraint
        as binary reward.

      $$r_\lambda(\xi) \triangleq \lambda \cdot 1[\xi \in \varphi]$$

      %ul
        %li
          By adding history to state space, can reduce to
          Maximum Causal Entropy Inverse Reinforcement Learning.
        %li
          <strong>Problem:</strong> Potential combinatorial explosion.
        %li
          <strong>Solution:</strong> Encode MDP as a Binary Decision
          Diagram.

      %ol
        %li
          Write the <strong>composition</strong> of the dynamics and
          property as a circuit with access to biased
          coins. 

          %figure
            %img(src="imgs/mdp_circ_unrolled.svg")/

        %li
          Can represent MDP with a Binary Decision Diagram:
          %figure#bdds
            %img(src="imgs/bdd.svg")/
            %img(src="imgs/bdd_cgraph.svg")/

          <strong>Conservative size bound:</strong>

          $$O(|\text{horizon}|\cdot |S/\varphi|\cdot |\text{Actions}|\log(|\text{Actions}|))$$
        %li
          We show you can efficiently compute maximum causal entropy 
          policy on compressed MDP.

      %p
        <strong>Application:</strong> Used to learn temporal logic constraint
        from <strong>unlabeled</strong> demonstrations, e.g.,

      %p#spec-example
        φ = "Avoid Lava, eventually recharge, and don't recharge while wet."

    %section
      %h1 Experiment

      %div#demos
        %figure
          %img(src="imgs/demos.svg")/

        %div
          %p Provided <strong>6</strong> unlabeled demonstrations.
          %ol
            %li Go to and stay at the <span class="outlined-text" style="color: #ffff00">yellow</span> tile.
            %li Avoid <span class="outlined-text" style="color: #ff8b8b">red</span> tiles.
            %li If you enter a <span class="outlined-text" style="color: #afafff">blue</span>, touch a <span class="outlined-text" style="color: sandybrown">brown</span> tile <strong>before</strong> recharging.

          %ul
            %li A = {↑, ↓, ←, →}.
            %li \(p = \frac{1}{32}\), slip and move ←.


  %footer
